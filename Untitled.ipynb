{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "285ce7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/jeongwonkim10516/nlp-fake-news-with-bert-99-55-top1\n",
    "# !pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "402af226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/israel/anaconda3/envs/pytorch/lib/python3.7/site-packages (4.6.0.dev0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/israel/anaconda3/envs/pytorch/lib/python3.7/site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: importlib-metadata in /home/israel/anaconda3/envs/pytorch/lib/python3.7/site-packages (from transformers) (3.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/israel/.local/lib/python3.7/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: filelock in /home/israel/anaconda3/envs/pytorch/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/israel/.local/lib/python3.7/site-packages (from transformers) (4.60.0)\n",
      "Requirement already satisfied: sacremoses in /home/israel/anaconda3/envs/pytorch/lib/python3.7/site-packages (from transformers) (0.0.44)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/israel/anaconda3/envs/pytorch/lib/python3.7/site-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: packaging in /home/israel/anaconda3/envs/pytorch/lib/python3.7/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: requests in /home/israel/anaconda3/envs/pytorch/lib/python3.7/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/israel/anaconda3/envs/pytorch/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/israel/anaconda3/envs/pytorch/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/israel/anaconda3/envs/pytorch/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/israel/anaconda3/envs/pytorch/lib/python3.7/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/israel/anaconda3/envs/pytorch/lib/python3.7/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/israel/anaconda3/envs/pytorch/lib/python3.7/site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/israel/anaconda3/envs/pytorch/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: six in /home/israel/anaconda3/envs/pytorch/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /home/israel/.local/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /home/israel/.local/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d27fe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b69b6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/israel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/israel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/israel/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/israel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0aa0ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from collections import Counter, defaultdict\n",
    "from PIL import Image\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "# Core packages for general use throughout the notebook.\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# For customizing our plots.\n",
    "\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Loading pytorch packages.\n",
    "\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ff94da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting some options for general use.\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set(font_scale=1.5)\n",
    "pd.options.display.max_columns = 250\n",
    "pd.options.display.max_rows = 250\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "#Setting seeds for consistent results.\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a558e246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 580\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.  \n",
    "    \n",
    "    device = torch.device('cuda')    \n",
    "\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4477c744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0728a55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training tweets: 20800\n",
      "\n",
      "Number of training tweets: 5200\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14649</th>\n",
       "      <td>14649</td>\n",
       "      <td>See Real Voting System Rigged For Election The...</td>\n",
       "      <td>Activist Post</td>\n",
       "      <td>By Bev Harris A real-time demo of the most dev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9231</th>\n",
       "      <td>9231</td>\n",
       "      <td>Selected Articles: Trump, the “Alt-right”, and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nBy Peter Koenig , November 15 2016 \\nThe ele...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6473</th>\n",
       "      <td>6473</td>\n",
       "      <td>Donald Trump Says He May Keep Parts of Obama H...</td>\n",
       "      <td>Reed Abelson</td>\n",
       "      <td>Just days after a national campaign in which h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18736</th>\n",
       "      <td>18736</td>\n",
       "      <td>Nunes ’Unmasking’ Report Vindicates Trump Clai...</td>\n",
       "      <td>Joel B. Pollak</td>\n",
       "      <td>House Intelligence Committee chair Rep. Devin ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12347</th>\n",
       "      <td>12347</td>\n",
       "      <td>Hillary’s Secret Is Out With What Camera Caugh...</td>\n",
       "      <td>Amanda Shea</td>\n",
       "      <td>Hillary’s Secret Is Out With What Camera Caugh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17157</th>\n",
       "      <td>17157</td>\n",
       "      <td>American Destroyer Fires Warning Shots at Iran...</td>\n",
       "      <td>Michael R. Gordon</td>\n",
       "      <td>WASHINGTON  —   In a vivid illustration of the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14974</th>\n",
       "      <td>14974</td>\n",
       "      <td>North Korea Threatens ‘Sacred’ Nuclear War Aga...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Email \\nNorth Korea’s Foreign Ministry slammed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11150</th>\n",
       "      <td>11150</td>\n",
       "      <td>BREAKING! NYPD Ready To Make Arrests In Weiner...</td>\n",
       "      <td>Fed Up</td>\n",
       "      <td>BREAKING! NYPD Ready To Make Arrests In Weiner...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2174</th>\n",
       "      <td>2174</td>\n",
       "      <td>These People Reversed Their Diabetes In 30 Days</td>\n",
       "      <td>REALdeal</td>\n",
       "      <td>Diabetes is one of the most rampant diseases o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>1504</td>\n",
       "      <td>Alabama Prison Officials Retaliate Against Pri...</td>\n",
       "      <td>Brian Sonenstein</td>\n",
       "      <td>Advocates say prison officials at the Kilby Co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "14649  14649  See Real Voting System Rigged For Election The...   \n",
       "9231    9231  Selected Articles: Trump, the “Alt-right”, and...   \n",
       "6473    6473  Donald Trump Says He May Keep Parts of Obama H...   \n",
       "18736  18736  Nunes ’Unmasking’ Report Vindicates Trump Clai...   \n",
       "12347  12347  Hillary’s Secret Is Out With What Camera Caugh...   \n",
       "17157  17157  American Destroyer Fires Warning Shots at Iran...   \n",
       "14974  14974  North Korea Threatens ‘Sacred’ Nuclear War Aga...   \n",
       "11150  11150  BREAKING! NYPD Ready To Make Arrests In Weiner...   \n",
       "2174    2174    These People Reversed Their Diabetes In 30 Days   \n",
       "1504    1504  Alabama Prison Officials Retaliate Against Pri...   \n",
       "\n",
       "                  author                                               text  \\\n",
       "14649      Activist Post  By Bev Harris A real-time demo of the most dev...   \n",
       "9231                 NaN  \\nBy Peter Koenig , November 15 2016 \\nThe ele...   \n",
       "6473        Reed Abelson  Just days after a national campaign in which h...   \n",
       "18736     Joel B. Pollak  House Intelligence Committee chair Rep. Devin ...   \n",
       "12347        Amanda Shea  Hillary’s Secret Is Out With What Camera Caugh...   \n",
       "17157  Michael R. Gordon  WASHINGTON  —   In a vivid illustration of the...   \n",
       "14974                NaN  Email \\nNorth Korea’s Foreign Ministry slammed...   \n",
       "11150             Fed Up  BREAKING! NYPD Ready To Make Arrests In Weiner...   \n",
       "2174            REALdeal  Diabetes is one of the most rampant diseases o...   \n",
       "1504    Brian Sonenstein  Advocates say prison officials at the Kilby Co...   \n",
       "\n",
       "       label  \n",
       "14649      1  \n",
       "9231       1  \n",
       "6473       0  \n",
       "18736      0  \n",
       "12347      1  \n",
       "17157      0  \n",
       "14974      1  \n",
       "11150      1  \n",
       "2174       1  \n",
       "1504       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "print(f'Number of training tweets: {train.shape[0]}\\n')\n",
    "print(f'Number of training tweets: {test.shape[0]}\\n')\n",
    "\n",
    "display(train.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf2ab018",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train['label'].values\n",
    "idx = len(labels)\n",
    "combined = pd.concat([train, test])\n",
    "combined = combined.fillna('no data')\n",
    "df = combined['title'] + ' ' + combined['author']\n",
    "combined = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d886d8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['House Dem Aide: We Didn’t Even See Comey’s Letter Until Jason Chaffetz Tweeted It Darrell Lucus',\n",
       "       'FLYNN: Hillary Clinton, Big Woman on Campus - Breitbart Daniel J. Flynn',\n",
       "       'Why the Truth Might Get You Fired Consortiumnews.com', ...,\n",
       "       'California Today: What, Exactly, Is in Your Sushi? - The New York Times Mike McPhate',\n",
       "       '300 US Marines To Be Deployed To Russian Border In Norway no data',\n",
       "       'Awkward Sex, Onscreen and Off - The New York Times Teddy Wayne'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67698f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6e8ca49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  House Dem Aide: We Didn’t Even See Comey’s Letter Until Jason Chaffetz Tweeted It Darrell Lucus\n",
      "Tokenized:  ['house', 'dem', 'aide', ':', 'we', 'didn', '’', 't', 'even', 'see', 'come', '##y', '’', 's', 'letter', 'until', 'jason', 'cha', '##ffe', '##tz', 't', '##wee', '##ted', 'it', 'darrell', 'luc', '##us']\n",
      "Token IDs:  [2160, 17183, 14895, 1024, 2057, 2134, 1521, 1056, 2130, 2156, 2272, 2100, 1521, 1055, 3661, 2127, 4463, 15775, 16020, 5753, 1056, 28394, 3064, 2009, 23158, 12776, 2271]\n"
     ]
    }
   ],
   "source": [
    "print(' Original: ', combined[0])\n",
    "print('Tokenized: ', tokenizer.tokenize(combined[0]))\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(combined[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89ff46a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  111\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for text in combined:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    \n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    \n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31b31c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "token_lens = []\n",
    "\n",
    "for text in combined:\n",
    "    tokens = tokenizer.encode(text, max_length = 512)\n",
    "    token_lens.append(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b649eed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train= combined[:idx]\n",
    "test = combined[idx:]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f55ed946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_map(sentence,labs='None'):\n",
    "    \n",
    "    \"\"\"A function for tokenize all of the sentences and map the tokens to their word IDs.\"\"\"\n",
    "    \n",
    "    global labels\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    \n",
    "    for text in sentence:\n",
    "        #   \"encode_plus\" will:\n",
    "        \n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        \n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            text,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            truncation='longest_first', # Activate and control truncation\n",
    "                            max_length = 84,           # Max length according to our text data.\n",
    "                            pad_to_max_length = True, # Pad & truncate all sentences.\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the id list. \n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        \n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    \n",
    "    if labs != 'None': # Setting this for using this definition for both train and test data so labels won't be a problem in our outputs.\n",
    "        labels = torch.tensor(labels)\n",
    "        return input_ids, attention_masks, labels\n",
    "    else:\n",
    "        return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d0e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_masks, labels = tokenize_map(train, labels)\n",
    "test_input_ids, test_attention_masks= tokenize_map(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3acb6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the training inputs into a TensorDataset.\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 80-20 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7863f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 15\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b03eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data = TensorDataset(test_input_ids, test_attention_masks)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6ff881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-large-uncased', # Use the 124-layer, 1024-hidden, 16-heads, 340M parameters BERT model with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification. You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the device which we set GPU in our case.\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c277225b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
